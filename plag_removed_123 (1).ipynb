{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing the **necessary** libraries"
      ],
      "metadata": {
        "id": "WZf6vJAu5HdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import xml.etree.ElementTree as ET\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-05-20T10:37:36.811595Z",
          "iopub.execute_input": "2023-05-20T10:37:36.811941Z",
          "iopub.status.idle": "2023-05-20T10:38:05.081418Z",
          "shell.execute_reply.started": "2023-05-20T10:37:36.811914Z",
          "shell.execute_reply": "2023-05-20T10:38:05.080414Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "_d9ULqjg0J3t",
        "outputId": "a94898a6-3650-40af-fe5b-56e37d19cbe7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among all the libraries only wandb needs to be installed on notebook online as it loses the session"
      ],
      "metadata": {
        "id": "Wo4At9k5uMI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wandb"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-12T03:18:20.243879Z",
          "iopub.execute_input": "2023-05-12T03:18:20.244310Z",
          "iopub.status.idle": "2023-05-12T03:18:39.601730Z",
          "shell.execute_reply.started": "2023-05-12T03:18:20.244270Z",
          "shell.execute_reply": "2023-05-12T03:18:39.600246Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db-P5d3n0J3y",
        "outputId": "5f4d2650-5fca-4a7d-90ed-78a318786d7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.23.1-py2.py3-none-any.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=f0c29474727faa06c555081a07352a88bd370a557b24a35e31f5e89d60b98eb1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.23.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Uploading the dataset\n",
        "Path needs to be mentioned properly "
      ],
      "metadata": {
        "id": "iPAJU2X35X4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"KEY : a1d8ccb0c91879ee96665642bf53e088782f937b  \"\"\"\n",
        "\n",
        "\n",
        "\"\"\"                            Uploading the data               \"\"\"\n",
        "\n",
        "CSV_file = pd.read_csv('/content/hin_train.csv')\n",
        "#print(CSV_file)\n",
        "\n",
        "\"\"\"                           Loading the Dataset                         \"\"\"\n",
        "\n",
        "X_train = CSV_file.iloc[:, 0].values\n",
        "#print(X_train)\n",
        "Y_train = CSV_file.iloc[:, 1].values\n",
        "#print(Y_train)\n",
        "\n",
        "CSV_file_test = pd.read_csv('/content/hin_valid.csv')\n",
        "#print(CSV_file)\n",
        "\n",
        "\"\"\"                           Loading the Dataset                         \"\"\"\n",
        "\n",
        "X_test = CSV_file_test.iloc[:, 0].values\n",
        "#print(X_train)\n",
        "Y_test = CSV_file_test.iloc[:, 1].values\n",
        "#print(Y_train)\n",
        "\n",
        "print(\" f1 Number of training samples: \", len(X_train))\n",
        "print(\"f1 Number of test samples: \", len(X_test))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:51:27.975921Z",
          "iopub.execute_input": "2023-05-20T10:51:27.976291Z",
          "iopub.status.idle": "2023-05-20T10:51:28.097586Z",
          "shell.execute_reply.started": "2023-05-20T10:51:27.976261Z",
          "shell.execute_reply": "2023-05-20T10:51:28.096536Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwGJly-30J3y",
        "outputId": "71c11f43-a4bc-4553-ff8e-8d0550f63cb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " f1 Number of training samples:  51199\n",
            "f1 Number of test samples:  4095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the data and creating the dictionary"
      ],
      "metadata": {
        "id": "MzYpm7bu6j6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# english and hindi dictionary\n",
        "hindi_ALPHA = [chr(alpha) for alpha in range(2304, 2432)]\n",
        "train_data = Data(X_train,Y_train)\n",
        "test_data = Data(X_test,Y_test)\n",
        "hindi_alphabets_ = hindi_ALPHA\n",
        "hindi_alphabets =hindi_alphabets_\n",
        "device_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# from a - z creating a array\n",
        "eng_alphabets = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "#total number of charchters in english and hindi vocabulary\n",
        "total_character_in_english = 26\n",
        "total_character_in_hindi = 129\n",
        "pad_char = '-P-'\n",
        "\n",
        "eng_index = {pad_char: 0}\n",
        "\n",
        "# making the dictionary for the english and hindi\n",
        "for i, c in enumerate(eng_alphabets):\n",
        "    print(i+1,\" \",end=\"\")\n",
        "    eng_index[c] = i+1\n",
        "    print(eng_index[c],\" \",end=\"\")\n",
        "print(eng_index)\n",
        "\n",
        "size_b = 0 \n",
        "intial_size = 0\n",
        "hindi_size = len(hindi_alphabets) + size_b - intial_size\n",
        "print(hindi_size)\n",
        "hindi_index = {pad_char: 0}\n",
        "\n",
        "for i, c in enumerate(hindi_alphabets):\n",
        "    #print(i+1,\" \",end=\"\")\n",
        "    hindi_index[c] = i+1\n",
        "    #print(hindi_index[c],\" \",end=\"\")\n",
        "print(hindi_index)\n",
        "\n",
        "#converting into english vector representation\n",
        "\n",
        "def word_representation(intial_word,word_size, word, l2i, device = 'cpu'):\n",
        "    temp_ = len(word)+1\n",
        "    temp_1 = len(l2i)\n",
        "    rep = torch.zeros(temp_, 1, temp_1).to(device)\n",
        "    special_zero = 0\n",
        "    for letter_index_to_another, letter in enumerate(word):\n",
        "        pos = l2i[letter]\n",
        "        #print(\"convert\")\n",
        "        rep[letter_index_to_another][special_zero][pos] = 1\n",
        "    pad_pos = l2i[pad_char]\n",
        "    #print(pad_pos)\n",
        "    rep[letter_index_to_another+1][special_zero][pad_pos] = 1\n",
        "    return rep\n",
        "value_of_max = 50\n",
        "#converting into hindi vector representation\n",
        "def transform_representation(intial_word,word_size,word, l2i, device = 'cpu'):\n",
        "    one = 1\n",
        "    #print(one)\n",
        "    temp_ = [len(word)+1, one]\n",
        "    ch = pad_char\n",
        "    word_represent = torch.zeros(temp_, dtype=torch.long).to(device)\n",
        "    for letter_index_to_another, letter in enumerate(word):\n",
        "        pos = l2i[letter]\n",
        "        zero = 0\n",
        "        word_represent[letter_index_to_another][zero] = pos\n",
        "    word_represent[letter_index_to_another+1][zero] = l2i[ch]\n",
        "    #print(\"converted\",one)\n",
        "    return word_represent\n",
        "\n",
        "\n",
        "eng, hindi = X_train,Y_train\n",
        "print(eng[0])\n",
        "eng_rep = word_representation('-p-',26,eng[0], eng_index)\n",
        "print(eng, eng_rep)\n",
        "print(hindi[0])\n",
        "hindi_gt = transform_representation('-p-',129,hindi[0], hindi_index)\n",
        "print(hindi, hindi_gt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:51:34.888412Z",
          "iopub.execute_input": "2023-05-20T10:51:34.888775Z",
          "iopub.status.idle": "2023-05-20T10:51:35.133080Z",
          "shell.execute_reply.started": "2023-05-20T10:51:34.888745Z",
          "shell.execute_reply": "2023-05-20T10:51:35.131933Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBe7BjY10J30",
        "outputId": "e497c372-bac5-4927-ce38-a25bdcdf00ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9  10  10  11  11  12  12  13  13  14  14  15  15  16  16  17  17  18  18  19  19  20  20  21  21  22  22  23  23  24  24  25  25  26  26  {'-P-': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "128\n",
            "{'-P-': 0, 'ऀ': 1, 'ँ': 2, 'ं': 3, 'ः': 4, 'ऄ': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ऌ': 13, 'ऍ': 14, 'ऎ': 15, 'ए': 16, 'ऐ': 17, 'ऑ': 18, 'ऒ': 19, 'ओ': 20, 'औ': 21, 'क': 22, 'ख': 23, 'ग': 24, 'घ': 25, 'ङ': 26, 'च': 27, 'छ': 28, 'ज': 29, 'झ': 30, 'ञ': 31, 'ट': 32, 'ठ': 33, 'ड': 34, 'ढ': 35, 'ण': 36, 'त': 37, 'थ': 38, 'द': 39, 'ध': 40, 'न': 41, 'ऩ': 42, 'प': 43, 'फ': 44, 'ब': 45, 'भ': 46, 'म': 47, 'य': 48, 'र': 49, 'ऱ': 50, 'ल': 51, 'ळ': 52, 'ऴ': 53, 'व': 54, 'श': 55, 'ष': 56, 'स': 57, 'ह': 58, 'ऺ': 59, 'ऻ': 60, '़': 61, 'ऽ': 62, 'ा': 63, 'ि': 64, 'ी': 65, 'ु': 66, 'ू': 67, 'ृ': 68, 'ॄ': 69, 'ॅ': 70, 'ॆ': 71, 'े': 72, 'ै': 73, 'ॉ': 74, 'ॊ': 75, 'ो': 76, 'ौ': 77, '्': 78, 'ॎ': 79, 'ॏ': 80, 'ॐ': 81, '॑': 82, '॒': 83, '॓': 84, '॔': 85, 'ॕ': 86, 'ॖ': 87, 'ॗ': 88, 'क़': 89, 'ख़': 90, 'ग़': 91, 'ज़': 92, 'ड़': 93, 'ढ़': 94, 'फ़': 95, 'य़': 96, 'ॠ': 97, 'ॡ': 98, 'ॢ': 99, 'ॣ': 100, '।': 101, '॥': 102, '०': 103, '१': 104, '२': 105, '३': 106, '४': 107, '५': 108, '६': 109, '७': 110, '८': 111, '९': 112, '॰': 113, 'ॱ': 114, 'ॲ': 115, 'ॳ': 116, 'ॴ': 117, 'ॵ': 118, 'ॶ': 119, 'ॷ': 120, 'ॸ': 121, 'ॹ': 122, 'ॺ': 123, 'ॻ': 124, 'ॼ': 125, 'ॽ': 126, 'ॾ': 127, 'ॿ': 128}\n",
            "bindhya\n",
            "['bindhya' 'kirankant' 'yagyopaveet' ... 'asahmaton' 'sulgaayin'\n",
            " 'anchuthengu'] tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
            "बिन्द्या\n",
            "['बिन्द्या' 'किरणकांत' 'यज्ञोपवीत' ... 'असहमतों' 'सुलगायीं' 'अंचुतेंगु'] tensor([[45],\n",
            "        [64],\n",
            "        [41],\n",
            "        [78],\n",
            "        [39],\n",
            "        [78],\n",
            "        [48],\n",
            "        [63],\n",
            "        [ 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class **DATA** : Used for preprocessing the data and handle the batch maintaining the randomness and implemented various functions"
      ],
      "metadata": {
        "id": "tzHdINx6_CP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class DATA : used for preprocess the data\n",
        "class Data(Dataset):\n",
        "    # constructor to load the data first \n",
        "\n",
        "    def __init__(self, X_train,Y_train):\n",
        "        self.hindi_words = Y_train\n",
        "        self.eng_words  = X_train\n",
        "        length_ = len(self.eng_words)\n",
        "        master = list(range(length_))\n",
        "        self.shuffle_indices = master\n",
        "        self.index_to_start = 0\n",
        "        random.shuffle(self.shuffle_indices)\n",
        "\n",
        "    # randomly picking the data point help to create a batch    \n",
        "    def __getitem__(self, index):\n",
        "        a= []\n",
        "        b = []\n",
        "        word_of_english = self.eng_words[index] \n",
        "        word_of_hindi = self.hindi_words[index]\n",
        "        return word_of_english, word_of_hindi    \n",
        "    \n",
        "    # return the desired item given the index as parameter from the dataset\n",
        "    def return_item(self, index):\n",
        "        english = self.eng_words[index]\n",
        "        hindi= self.hindi_words[index]\n",
        "        return english,hindi \n",
        "    \n",
        "    # returning the batch of 2 words in our case it is english and hindi\n",
        "    def func_batch(self, batch_size, array,x_,y_):\n",
        "        batch = []\n",
        "        batch_ = batch_size\n",
        "        self.zero = 0\n",
        "        end = self.index_to_start + batch_\n",
        "        \n",
        "        if end >= len(self.eng_words) and self.zero != batch_:\n",
        "            batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_words)]]\n",
        "            self.zero = (batch_ - 1)\n",
        "            self.start_batch_to_array= []\n",
        "            end = len(self.eng_words)\n",
        "        result = batch.copy()\n",
        "        for i in range(self.shuffle_start_index, end):\n",
        "            result.append(array[self.shuffle_indices[i]])\n",
        "        return result\n",
        "    \n",
        "    # given a batch return those many number of data\n",
        "    def get_batch(self, batch_size, postprocess = True):\n",
        "        eng_batch = self.func_batch(batch_size, self.eng_words,0,1)\n",
        "        hindi_batch = self.func_batch(batch_size, self.hindi_words,0,1)\n",
        "        self.shuffle_start_index = self.shuffle_start_index + batch_size \n",
        "        self.shuffle_start_index += 1\n",
        "        batch_ = 1\n",
        "        if self.shuffle_start_index >= len(self.eng_words) and self.zero != len(self.eng_words):\n",
        "            random.shuffle(self.shuffle_indices)\n",
        "            self.zero = (batch_ - 1)\n",
        "            self.shuffle_start_index = 0\n",
        "\n",
        "        # return the batch    \n",
        "        return eng_batch, hindi_batch"
      ],
      "metadata": {
        "id": "l9dq6WPj-_9_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Decoder Class which taked various parametera as input GRU and RNN is combined in this class"
      ],
      "metadata": {
        "id": "c3Qdn1yBus8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple Encoder _Decoder Class\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    #constructor\n",
        "    def __init__(self,model_name, input_size, hidden_size, output_size, num_layers=1, dropout=0, bidirectional=False, verbose=False):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        if(model_name == \"RNN\"):\n",
        "            self.encoder = nn.RNN(input_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "            self.decoder = nn.RNN(output_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        if(model_name == \"GRU\"):\n",
        "            self.encoder = nn.GRU(input_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "            self.decoder = nn.GRU(output_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        two = 2\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=two)\n",
        "        \n",
        "    #forward loop    \n",
        "    def forward(self, input, max_output_chars = value_of_max, device = 'cpu', ground_truth = None):\n",
        "        one = 1\n",
        "        # encoder\n",
        "        vector = []\n",
        "        out, hidden = self.encoder(input)\n",
        "        temp_out = out\n",
        "        # decoder\n",
        "        decoder_state = hidden\n",
        "        din = decoder_state\n",
        "        decoder_input = torch.zeros(one, one, self.output_size).to(device)\n",
        "        outputs = []\n",
        "        one_ = 1\n",
        "        for i in range(max_output_chars):\n",
        "            \n",
        "            out, decoder_state = self.decoder(decoder_input, decoder_state)\n",
        "            \n",
        "            out = self.h2o(decoder_state)\n",
        "            temp_answer = out  \n",
        "            out = self.softmax(out)\n",
        "            temp_answer = out  \n",
        "            outputs.append(out.view(one, -1 * one))\n",
        "            \n",
        "            maximum_index = torch.argmax(out, one+one, keepdim=True)\n",
        "            if not ground_truth is None and one == 1:\n",
        "                maximum_index = ground_truth[i+one-one].reshape(one, one, one)\n",
        "            one_hot_vector = torch.FloatTensor(out.shape).to(device)\n",
        "            one_hot_vector.zero_()\n",
        "            one_hot_vector.scatter_(one+one, maximum_index, one)\n",
        "            \n",
        "            decoder_input = one_hot_vector.detach()\n",
        "            decoder_temp = decoder_input\n",
        "        #returning results \n",
        "        return outputs\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:51:40.864956Z",
          "iopub.execute_input": "2023-05-20T10:51:40.865313Z",
          "iopub.status.idle": "2023-05-20T10:51:40.879334Z",
          "shell.execute_reply.started": "2023-05-20T10:51:40.865282Z",
          "shell.execute_reply": "2023-05-20T10:51:40.878050Z"
        },
        "trusted": true,
        "id": "rE1eXkfI0J31"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Class "
      ],
      "metadata": {
        "id": "QX83z0Dvu4ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM class\n",
        "class EncoderDecoder_LSTM(nn.Module):\n",
        "    # constructor\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0, bidirectional=False, verbose=False):\n",
        "        super(EncoderDecoder_LSTM, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        self.decoder = nn.LSTM(output_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    #forward loop    \n",
        "    def forward(self, input, max_output_chars =value_of_max, device = 'cpu', ground_truth = None):\n",
        "        two = 2\n",
        "        zero = 0\n",
        "        # encoder\n",
        "        out, (hidden, cell_state) = self.encoder(input)\n",
        "        one = 1\n",
        "        minone = -1\n",
        "        # decoder\n",
        "        decoder_state = (hidden, cell_state)\n",
        "        decoder_input = torch.zeros(two - one, one, self.output_size).to(device)\n",
        "        outputs = []\n",
        "        \n",
        "        for i in range(0,max_output_chars,1):\n",
        "            \n",
        "            out, decoder_state = self.decoder(decoder_input, decoder_state)\n",
        "            temp_out  = out\n",
        "            out = self.h2o(decoder_state[zero])\n",
        "            answer = out\n",
        "            out = self.softmax(out)\n",
        "            temp_out = answer\n",
        "            count = 1\n",
        "            outputs.append(out.view(one, minone+zero))\n",
        "            \n",
        "            maximum_index = torch.argmax(out, two+zero, keepdim=True)\n",
        "            indexing_value = maximum_index+1\n",
        "            if not ground_truth is None and zero == one+minone and two == 2:\n",
        "                iterator = i\n",
        "                maximum_index = ground_truth[iterator].reshape(one, two - one, one+zero)\n",
        "            one_hot_vector = torch.FloatTensor(out.shape).to(device)\n",
        "            one_hot_vector.zero_()\n",
        "            vector_temp  = one_hot_vector \n",
        "            one_hot_vector.scatter_(two+zero, maximum_index, two - one)\n",
        "            \n",
        "            decoder_input = one_hot_vector.detach()\n",
        "            din_in = decoder_input\n",
        "        return outputs\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:51:44.462341Z",
          "iopub.execute_input": "2023-05-20T10:51:44.462707Z",
          "iopub.status.idle": "2023-05-20T10:51:44.476704Z",
          "shell.execute_reply.started": "2023-05-20T10:51:44.462671Z",
          "shell.execute_reply": "2023-05-20T10:51:44.475437Z"
        },
        "trusted": true,
        "id": "KhYOYQCD0J31"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Classes of GRU and RNN"
      ],
      "metadata": {
        "id": "e1B0QNW5u9lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder_Attention(nn.Module):\n",
        "    \n",
        "    def __init__(self,model_type, input_size, hidden_size, output_size, num_layers=1, dropout=0, bidirectional=False, verbose=False):\n",
        "        super(EncoderDecoder_Attention, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        if(model_type == \"GRU\"):\n",
        "            self.encoder = nn.GRU(input_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "            self.decoder = nn.GRU(hidden_size*2, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        if(model_type == \"RNN\"):\n",
        "            self.encoder = nn.RNN(input_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "            self.decoder = nn.RNN(hidden_size*2, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "        self.U = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.W = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size, 1)\n",
        "        self.out2hidden = nn.Linear(self.output_size, self.hidden_size)   \n",
        "        \n",
        "        \n",
        "    def forward(self, input, max_output_chars = value_of_max, device = 'cpu', ground_truth = None):\n",
        "        \n",
        "        # encoder\n",
        "        one = 1\n",
        "        two = 2\n",
        "        encoder_outputs, hidden = self.encoder(input)\n",
        "        out_e  = encoder_outputs\n",
        "        encoder_outputs = encoder_outputs.view(one - two, self.hidden_size)\n",
        "\n",
        "        decoder_state = hidden\n",
        "        decoder_input = torch.zeros(two - one, one, self.output_size).to(device)\n",
        "        zero = 0\n",
        "        \n",
        "        output_answer = []\n",
        "        predict = []\n",
        "        attention = []\n",
        "        U = self.U(encoder_outputs)\n",
        "        count = 1\n",
        "        for i in range(0,max_output_chars,1):\n",
        "            \n",
        "            W = self.W(decoder_state.view(one, one-two).repeat(encoder_outputs.shape[zero], one+zero))\n",
        "            V = self.attn(torch.tanh(U + W))\n",
        "            attn_weights = F.softmax(V.view(one, zero - one), dim = two - one) \n",
        "            \n",
        "            attn_applied = torch.bmm(attn_weights.unsqueeze(zero),\n",
        "                                 encoder_outputs.unsqueeze(zero))\n",
        "            \n",
        "            embedding = self.out2hidden(decoder_input)\n",
        "            decoder_input = torch.cat((embedding[two -two], attn_applied[zero]), zero+one).unsqueeze(zero)\n",
        "                \n",
        "            out, decoder_state = self.decoder(decoder_input, decoder_state)\n",
        "            temp_out = out\n",
        "            di_state = decoder_state    \n",
        "            out = self.h2o(decoder_state)\n",
        "            answer_out = out\n",
        "            out = self.softmax(out)\n",
        "            temp = temp_out\n",
        "            count = (1+ count)%10\n",
        "            output_answer.append(out.view(one, one-two))\n",
        "            \n",
        "            index_of_array = i\n",
        "            maximum_index = torch.argmax(out, two, keepdim=True)\n",
        "            if not ground_truth is None and one == one+zero:\n",
        "                maximum_index = ground_truth[index_of_array].reshape(one+zero, one, two-one)\n",
        "            one_hot_vector = torch.zeros(out.shape, device=device)\n",
        "            one_hot_vector.scatter_(two, maximum_index, one)\n",
        "            decoder_input = one_hot_vector.detach()\n",
        "            din = decoder_input\n",
        "        \n",
        "        return output_answer\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:51:47.593409Z",
          "iopub.execute_input": "2023-05-20T10:51:47.593787Z",
          "iopub.status.idle": "2023-05-20T10:51:47.612192Z",
          "shell.execute_reply.started": "2023-05-20T10:51:47.593756Z",
          "shell.execute_reply": "2023-05-20T10:51:47.610122Z"
        },
        "trusted": true,
        "id": "FpTP6fUd0J32"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Attention Class"
      ],
      "metadata": {
        "id": "BOyb_7ZYvD9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder_Attention_LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.0, bidirectional=False, verbose=False):\n",
        "        super(EncoderDecoder_Attention_LSTM, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers=num_layers, \n",
        "                                        dropout=dropout, bidirectional=bidirectional)\n",
        "        self.decoder = nn.LSTM(hidden_size*2, hidden_size, num_layers=num_layers, \n",
        "                                        dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "        self.U = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.W = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size, 1)\n",
        "        self.out2hidden = nn.Linear(self.output_size, self.hidden_size)   \n",
        "        \n",
        "        \n",
        "    def forward(self, input, max_output_chars =value_of_max, device = 'cpu', ground_truth = None):\n",
        "        one = 1\n",
        "        minone = -1\n",
        "        zero = 0\n",
        "        # encoder\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(input)\n",
        "        two = 2\n",
        "        if self.bidirectional:\n",
        "            hidden = hidden.view(self.num_layers, two, one - two, self.hidden_size)\n",
        "            cell = cell.view(self.num_layers, two, minone, self.hidden_size)\n",
        "            # concatenate the hidden state of the last layer for the forward and backward LSTM\n",
        "            hidden = torch.cat((hidden[minone,zero], hidden[one-two,one]), dim=one).unsqueeze(zero)\n",
        "            cell = torch.cat((cell[zero - one,zero], cell[minone,one]), dim=two - one).unsqueeze(zero)\n",
        "        else:\n",
        "            hidden = hidden[minone,:,:].unsqueeze(two -two)\n",
        "            cell = cell[minone,:,:].unsqueeze(zero)\n",
        "        encoder_outputs = encoder_outputs.view(minone, self.hidden_size)\n",
        "        \n",
        "\n",
        "        \n",
        "        # decoder output \n",
        "        decoder_state = (hidden, cell)\n",
        "        decoder_input = torch.zeros(1, 1, self.output_size).to(device)\n",
        "        \n",
        "        outputs = []\n",
        "        U = self.U(encoder_outputs)\n",
        "        \n",
        "        for i in range(0,max_output_chars,1):\n",
        "            \n",
        "            W = self.W(decoder_state[zero].view(one, -1).repeat(encoder_outputs.shape[zero], one))\n",
        "            V = self.attn(torch.tanh(U + W))\n",
        "            attn_weights = F.softmax(V.view(one, minone), dim = one) \n",
        "            \n",
        "            attn_applied = torch.bmm(attn_weights.unsqueeze(zero),\n",
        "                                 encoder_outputs.unsqueeze(zero))\n",
        "            \n",
        "            embedding = self.out2hidden(decoder_input)\n",
        "            decoder_input = torch.cat((embedding[zero], attn_applied[zero]), one).unsqueeze(zero)\n",
        "\n",
        "                \n",
        "            out, decoder_state = self.decoder(decoder_input, decoder_state)\n",
        "                \n",
        "            out = self.h2o(decoder_state[zero])\n",
        "            temp_out = out\n",
        "            out = self.softmax(out)\n",
        "            temp_out = out\n",
        "            outputs.append(out.view(one, minone))\n",
        "            ans  = temp_out\n",
        "            \n",
        "            maximum_index = torch.argmax(out, two, keepdim=True)\n",
        "            if not ground_truth is None and zero == 0 and two == 2:\n",
        "                maximum_index = ground_truth[i].reshape(one, zero+one, two - one)\n",
        "            one_hot_vector = torch.zeros(out.shape, device=device)\n",
        "            temp_store = one_hot_vector\n",
        "            one_hot_vector.scatter_(two+zero,maximum_index, one+zero) \n",
        "            \n",
        "            decoder_input = one_hot_vector.detach()\n",
        "            din_in  = decoder_input\n",
        "        return outputs\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:51:51.156667Z",
          "iopub.execute_input": "2023-05-20T10:51:51.157380Z",
          "iopub.status.idle": "2023-05-20T10:51:51.176441Z",
          "shell.execute_reply.started": "2023-05-20T10:51:51.157346Z",
          "shell.execute_reply": "2023-05-20T10:51:51.175574Z"
        },
        "trusted": true,
        "id": "AUVRmgA80J32"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vector representation of English Word (Input language word)"
      ],
      "metadata": {
        "id": "MNr6ydQ3t9VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function which return the vector representation of the output\n",
        "\n",
        "\n",
        "def vector_representation(english_word,hindi_word,net, word,max_output_chars, device='cpu'):\n",
        "    net.eval().to(device)\n",
        "    temp_word = []\n",
        "    temp_word.append('@')\n",
        "    temp_word.append('#')\n",
        "    word_ohe = word_representation('pad',26,word, eng_index)\n",
        "    count = 1\n",
        "    count+=2\n",
        "    output = net(word_ohe, max_output_chars)\n",
        "    if(count > 4):\n",
        "        count+=1\n",
        "        print(count)\n",
        "    return output\n",
        "net = EncoderDecoder(\"GRU\",len(eng_index), 256, len(hindi_index), num_layers=2)\n",
        "     \n",
        "out = vector_representation('english','hindi',net, 'india', 30)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:51:55.769252Z",
          "iopub.execute_input": "2023-05-20T10:51:55.770019Z",
          "iopub.status.idle": "2023-05-20T10:51:55.888561Z",
          "shell.execute_reply.started": "2023-05-20T10:51:55.769986Z",
          "shell.execute_reply": "2023-05-20T10:51:55.887667Z"
        },
        "trusted": true,
        "id": "qfv_IN9l0J33"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "GMSM1Sant0Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training function takes the couple of parameter and train on GPU\n",
        "\n",
        "\n",
        "def training_model(algorithm,X_train, Y_train , net, lr = 0.01, no_of_batches = 100, batch_size = 10,  display_freq=5, device = 'cpu'):\n",
        "    momentum = 0.9\n",
        "    net = net.to(device)\n",
        "    criterion = nn.NLLLoss(ignore_index = -1)\n",
        "    if(algorithm == 'Adam') :   \n",
        "        opt = optim.Adam(net.parameters(), lr=lr)\n",
        "    if(algorithm == 'NAdam') :   \n",
        "        opt = optim.NAdam(net.parameters(), lr=lr)\n",
        "    if(algorithm == 'RMSprop'):    \n",
        "        opt = optim.RMSprop(net.parameters(), lr=lr)    \n",
        "    variable = 2\n",
        "    teacher_force_upto = no_of_batches//variable\n",
        "    \n",
        "    loss_arr = np.zeros(no_of_batches + 1)\n",
        "    one = 1\n",
        "    display = True\n",
        "    for i in range(0,no_of_batches,1):\n",
        "        value_stored = loss_arr[i]*i\n",
        "        X = i< teacher_force_upto\n",
        "        print(i)\n",
        "        loss_arr[i+1] = (value_stored + batch(X_train,Y_train,net, opt, criterion, batch_size, device = device, teacher_force = X))/(i+1)\n",
        "        acc = calc_accuracy(net, device)\n",
        "        acc_train = calc_accuracy_train(net, device)\n",
        "        #wandb.log({\"Loss\": loss_arr[i+1] , \"Validation accuracy\": acc,\"Training accuracy\": acc_train,\"Epochs\": i})\n",
        "        frequency = i%display_freq\n",
        "        if  frequency == display_freq-one and display == True:\n",
        "            clear_output(wait = True)\n",
        "            print('Iteration',i,'Loss',loss_arr[i],'accuracy',acc)\n",
        "            plt.figure()\n",
        "            plt.plot(loss_arr[1:i],'-*')\n",
        "            plt.xlabel('Iteration')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.show()\n",
        "            print('\\n')\n",
        "            \n",
        "    \n",
        "    return loss_arr"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:59:57.995631Z",
          "iopub.execute_input": "2023-05-20T10:59:57.995992Z",
          "iopub.status.idle": "2023-05-20T10:59:58.009158Z",
          "shell.execute_reply.started": "2023-05-20T10:59:57.995964Z",
          "shell.execute_reply": "2023-05-20T10:59:58.008219Z"
        },
        "trusted": true,
        "id": "3TJF5fOm0J33"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backward propagation"
      ],
      "metadata": {
        "id": "KjgsIZ0Ot4if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation function \n",
        "\n",
        "def batch(X_train ,Y_train ,net , opt, criterion, batch_size, device = 'cuda', teacher_force = False):\n",
        "    \n",
        "    net.train().to(device)\n",
        "    opt.zero_grad()\n",
        "    eng_batch, hindi_batch = X_train,Y_train\n",
        "    loss_array_pre = []\n",
        "    total_loss = 0\n",
        "    for i in range(0,batch_size,1):\n",
        "        \n",
        "        input = word_representation('pad',26,eng_batch[i], eng_index, device)\n",
        "        word= transform_representation('pad',129,hindi_batch[i], hindi_index, device)\n",
        "        word = transform_representation('pad',129,hindi_batch[i], hindi_index, device)\n",
        "        if(teacher_force == True):\n",
        "            value_of_ground = word\n",
        "        else:\n",
        "            value_of_ground = None\n",
        "        outputs = net(input, word.shape[0], device, ground_truth = value_of_ground)\n",
        "        \n",
        "        for index, output in enumerate(outputs):\n",
        "            temp_var =word[index]\n",
        "            temp_batch = batch_size\n",
        "            loss = criterion(output, temp_var) /temp_batch \n",
        "            loss.backward(retain_graph = True)\n",
        "            total_loss =total_loss + loss\n",
        "        \n",
        "    opt.step()\n",
        "    answer = total_loss/batch_size\n",
        "    a = 0\n",
        "    count = True\n",
        "    if(count == True):\n",
        "        a+=1\n",
        "        \n",
        "    return answer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:52:00.966838Z",
          "iopub.execute_input": "2023-05-20T10:52:00.967195Z",
          "iopub.status.idle": "2023-05-20T10:52:00.977651Z",
          "shell.execute_reply.started": "2023-05-20T10:52:00.967168Z",
          "shell.execute_reply": "2023-05-20T10:52:00.976561Z"
        },
        "trusted": true,
        "id": "VsdZYjs-0J33"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gives the output in hindi (My Chosen language ) from english"
      ],
      "metadata": {
        "id": "laGyb_0Y_fXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to take output in hindi just by giving english word after the training\n",
        "\n",
        "def hindi_output_word(net, word, device = 'cpu'):\n",
        "    hindi_to_english = ''\n",
        "    net = net.eval().to(device)\n",
        "    one = 1\n",
        "    outputs = vector_representation('english','hindi',net, word, 30, device)\n",
        "    hindi_output = ''\n",
        "    i = 0\n",
        "    zero = 0\n",
        "    out_temp = []\n",
        "    for out in outputs:\n",
        "        out_temp.append(i)\n",
        "        i+=1\n",
        "        val, indices = out.topk(one)\n",
        "        i-=1\n",
        "        out_temp.append(i)\n",
        "        index = indices.tolist()[zero][one-one]\n",
        "        if index == 0 and one == 1:\n",
        "            break\n",
        "        hindi_char = hindi_alphabets[index+1]\n",
        "        hindi_output =hindi_output + hindi_char\n",
        "    print(word + ' - ' + hindi_output)\n",
        "    return hindi_output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:52:03.433478Z",
          "iopub.execute_input": "2023-05-20T10:52:03.434448Z",
          "iopub.status.idle": "2023-05-20T10:52:03.442230Z",
          "shell.execute_reply.started": "2023-05-20T10:52:03.434402Z",
          "shell.execute_reply": "2023-05-20T10:52:03.441147Z"
        },
        "trusted": true,
        "id": "8yY0EDbc0J34"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculating the accuracies"
      ],
      "metadata": {
        "id": "he3eOtnB-x1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Below two function calcuates the accuracy of training and validation\n",
        "\n",
        "def calc_accuracy_train(net, device = 'cpu'):\n",
        "    zero = 0\n",
        "    net = net.eval().to(device)\n",
        "    outputs = []\n",
        "    predictions = []\n",
        "    accuracy = 0\n",
        "    percentage = 0\n",
        "    for i in range(0,1024,1):\n",
        "        eng, hindi = train_data[i]\n",
        "        english_word = eng\n",
        "        hindi_word = hindi\n",
        "        word = transform_representation('pad',129,hindi, hindi_index, device)\n",
        "        correct_words = np.array(5)\n",
        "        outputs = vector_representation('english','hindi',net, eng, word.shape[0])\n",
        "        correct = 0\n",
        "        sum = len(X_train)\n",
        "        for index, out in enumerate(outputs):\n",
        "            val, indices = out.topk(1)\n",
        "            sum+=correct\n",
        "            hindi_position = indices.tolist()[0]\n",
        "            if hindi_position[zero] == word[index][zero]:\n",
        "                correct = correct + 1\n",
        "        \n",
        "        accuracy =accuracy + (correct/word.shape[0])\n",
        "    sum = 0\n",
        "    accuracy =accuracy / 1024\n",
        "    return accuracy\n",
        "def calc_accuracy(net, device = 'cpu'):\n",
        "    zero = 0\n",
        "    net = net.eval().to(device)\n",
        "    outputs = []\n",
        "    predictions = []\n",
        "    accuracy = 0\n",
        "    percentage = 0\n",
        "    for i in range(0,1024,1):\n",
        "        eng, hindi = test_data[i]\n",
        "        english_word = eng\n",
        "        hindi_word = hindi\n",
        "        word = transform_representation('pad',129,hindi, hindi_index, device)\n",
        "        correct_words = np.array(5)\n",
        "        outputs = vector_representation('english','hindi',net, eng, word.shape[0])\n",
        "        correct = 0\n",
        "        sum = len(X_train)\n",
        "        for index, out in enumerate(outputs):\n",
        "            val, indices = out.topk(1)\n",
        "            sum+=correct\n",
        "            hindi_position = indices.tolist()[0]\n",
        "            if hindi_position[zero] == word[index][zero]:\n",
        "                correct = correct + 1\n",
        "        \n",
        "        accuracy =accuracy + (correct/word.shape[0])\n",
        "    sum = 0\n",
        "    accuracy =accuracy / 1024\n",
        "    return accuracy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:52:10.807333Z",
          "iopub.execute_input": "2023-05-20T10:52:10.807697Z",
          "iopub.status.idle": "2023-05-20T10:52:10.820162Z",
          "shell.execute_reply.started": "2023-05-20T10:52:10.807658Z",
          "shell.execute_reply": "2023-05-20T10:52:10.818988Z"
        },
        "trusted": true,
        "id": "9Tyyhmrv0J34"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T06:20:15.233192Z",
          "iopub.execute_input": "2023-05-20T06:20:15.233593Z",
          "iopub.status.idle": "2023-05-20T06:20:15.240668Z",
          "shell.execute_reply.started": "2023-05-20T06:20:15.233559Z",
          "shell.execute_reply": "2023-05-20T06:20:15.239770Z"
        },
        "trusted": true,
        "id": "YOnvWOJl0J35"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "net = EncoderDecoder_LSTM(len(eng_index), 256, len(hindi_index), num_layers=1,dropout =0)\n",
        "loss_history = training_model(\"Adam\",X_train,Y_train,net, lr=0.01, no_of_batches= 3, batch_size = 64, display_freq=10, device = device_gpu)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T06:08:23.553242Z",
          "iopub.execute_input": "2023-05-20T06:08:23.554342Z",
          "iopub.status.idle": "2023-05-20T06:09:50.980093Z",
          "shell.execute_reply.started": "2023-05-20T06:08:23.554294Z",
          "shell.execute_reply": "2023-05-20T06:09:50.979120Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jQH8Eek0J35",
        "outputId": "e4c41f97-fcf5-4d5b-e93d-2318b5dd1691"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sweeps of wandb (Vanilla Model)"
      ],
      "metadata": {
        "id": "uVTwkBPdxRAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  sweeps\n",
        "\n",
        "sweep_config = {\n",
        "  \"name\": \"Bayesian Sweep\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\":{\n",
        "  \"name\": \"WordLevel_Validation_Accuracy\",\n",
        "  \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"hidden_layer_size\": {\n",
        "            \"values\": [128,256,512]\n",
        "        },\n",
        "        \"num_encoder_layers\": {\n",
        "            \"values\": [1]\n",
        "        },\n",
        "         \"num_decoder_layers\": {\n",
        "            \"values\": [1]\n",
        "        },\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.005,0.01,0.05,0.02]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": ['Adam','RMSprop','NAdam']\n",
        "        },\n",
        "        \n",
        "        \"batch_size\": {\n",
        "            \"values\": [512,128,256,1024]\n",
        "        },\n",
        "        \n",
        "        \"model_name\": {\n",
        "            \"values\": [\"RNN\",\"GRU\",\"LSTM\"]\n",
        "        },\n",
        " \n",
        "        \n",
        "        \"dropout\": {\n",
        "            \"values\": [0.0,0.1,0.2,0.3]\n",
        "        },\n",
        "                    \n",
        "        \"epochs\": {\n",
        "            \"values\": [100,50,125,150]\n",
        "        },\n",
        "      \n",
        "        \n",
        "        \n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"Assignment3_CS6910\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T10:52:15.636184Z",
          "iopub.execute_input": "2023-05-20T10:52:15.636629Z",
          "iopub.status.idle": "2023-05-20T10:52:16.994097Z",
          "shell.execute_reply.started": "2023-05-20T10:52:15.636520Z",
          "shell.execute_reply": "2023-05-20T10:52:16.993184Z"
        },
        "trusted": true,
        "id": "pOooC5Zm0J35",
        "outputId": "8713fbbf-459d-4ec6-f448-340f4b3dcd85"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Create sweep with ID: 3hol7qes\nSweep URL: https://wandb.ai/cs6910_susmit/Assignment3_CS6910/sweeps/3hol7qes\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This sweep is for  Attention Encoder Decoder Model"
      ],
      "metadata": {
        "id": "iXIoU_Yrs9e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train function which is called by the wandb agent\n",
        "\n",
        "def train():\n",
        "    \n",
        "    config_defaults = {\n",
        "            \"epochs\": 400,\n",
        "            \"num_encoder_layers\": 1,\n",
        "            \"num_decoder_layers\": 1,\n",
        "            \"batch_size\":512,\n",
        "            \n",
        "            \"hidden_layer_size\":256,\n",
        "            \"learning_rate\":0.01,\n",
        "            \"algorithm\":'Adam',\n",
        "            \"dropout\":0.0,\n",
        "            \"model_name\" : \"GRU\"\n",
        "        }\n",
        "    \n",
        "    wandb.init(project=\"Assignment3_CS6910\",config=config_defaults)\n",
        "    config = wandb.config\n",
        "    batches = config.epochs\n",
        "    hidden_layer_size = config.hidden_layer_size\n",
        "    batch_size = config.batch_size\n",
        "    learning_rate = config.learning_rate\n",
        "    number_of_layer = config.num_encoder_layers\n",
        "    algorithm = config.algorithm\n",
        "    dropout = config.dropout\n",
        "    model = config.model_name\n",
        "    device_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net , loss_history = 0,0\n",
        "    if model != \"LSTM\":\n",
        "        net = EncoderDecoder_Attention(model,len(eng_index), hidden_layer_size, len(hindi_index), num_layers=1,dropout = dropout)\n",
        "        loss_history = training_model(algorithm,X_train,Y_train,net, lr=learning_rate, no_of_batches= batches, batch_size = batch_size, display_freq=10, device = device_gpu)\n",
        "    if model == \"LSTM\":\n",
        "        net = EncoderDecoder_Attention_LSTM(len(eng_index), hidden_layer_size, len(hindi_index), num_layers=1,dropout = dropout)\n",
        "        loss_history = training_model(algorithm,X_train,Y_train,net, lr=learning_rate, no_of_batches= batches, batch_size = batch_size, display_freq=10, device = device_gpu) \n",
        "    accuracy = calc_accuracy(net) \n",
        "\n",
        "    print('Accuracy w/o attention ', accuracy)    \n",
        "        \n",
        "\n",
        "# train function is simply called when i want only 1 choice of parameters to learn\n",
        "\n",
        "train()\n",
        "\n",
        "# wandb agents to run sweeps\n",
        "\n",
        "#wandb.agent(sweep_id,train,count = 10)\n",
        "        "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-20T11:56:10.370891Z",
          "iopub.execute_input": "2023-05-20T11:56:10.371261Z",
          "iopub.status.idle": "2023-05-20T15:07:36.540229Z",
          "shell.execute_reply.started": "2023-05-20T11:56:10.371231Z",
          "shell.execute_reply": "2023-05-20T15:07:36.536033Z"
        },
        "trusted": true,
        "id": "ERl3dLOT0J35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sweep over Simple Encoder Decoder Model"
      ],
      "metadata": {
        "id": "NWBUqfu0tfPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train function which is called by the wandb agent\n",
        "\n",
        "def train():\n",
        "    \n",
        "    config_defaults = {\n",
        "            \"epochs\": 400,\n",
        "            \"num_encoder_layers\": 1,\n",
        "            \"num_decoder_layers\": 1,\n",
        "            \"batch_size\":512,\n",
        "            \n",
        "            \"hidden_layer_size\":256,\n",
        "            \"learning_rate\":0.01,\n",
        "            \"algorithm\":'Adam',\n",
        "            \"dropout\":0.0,\n",
        "            \"model_name\" : \"GRU\"\n",
        "        }\n",
        "    \n",
        "    wandb.init(project=\"Assignment3_CS6910\",config=config_defaults)\n",
        "    config = wandb.config\n",
        "    batches = config.epochs\n",
        "    hidden_layer_size = config.hidden_layer_size\n",
        "    batch_size = config.batch_size\n",
        "    learning_rate = config.learning_rate\n",
        "    number_of_layer = config.num_encoder_layers\n",
        "    algorithm = config.algorithm\n",
        "    dropout = config.dropout\n",
        "    model = config.model_name\n",
        "    device_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net , loss_history = 0,0\n",
        "    if model != \"LSTM\":\n",
        "        net = EncoderDecoder(model,len(eng_index), hidden_layer_size, len(hindi_index), num_layers=1,dropout = dropout)\n",
        "        loss_history = training_model(algorithm,X_train,Y_train,net, lr=learning_rate, no_of_batches= batches, batch_size = batch_size, display_freq=10, device = device_gpu)\n",
        "    if model == \"LSTM\":\n",
        "        net = EncoderDecoder_LSTM(len(eng_index), hidden_layer_size, len(hindi_index), num_layers=1,dropout = dropout)\n",
        "        loss_history = training_model(algorithm,X_train,Y_train,net, lr=learning_rate, no_of_batches= batches, batch_size = batch_size, display_freq=10, device = device_gpu) \n",
        "    accuracy = calc_accuracy(net) \n",
        "\n",
        "    print('Accuracy w/o attention ', accuracy)    \n",
        "        \n",
        "\n",
        "# train function is simply called when i want only 1 choice of parameters to learn\n",
        "\n",
        "train()\n",
        "\n",
        "# wandb agents to run sweeps\n",
        "\n",
        "#wandb.agent(sweep_id,train,count = 10)\n",
        "        "
      ],
      "metadata": {
        "id": "PbfXoaXrta8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction for vanilla _Encoder _decoder"
      ],
      "metadata": {
        "id": "PurKVmOuJyNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "input_word_array = []\n",
        "count_ =20 # number of data points i want as representation \n",
        "correct_word = []\n",
        "number = 20\n",
        "predicted_word_array = []\n",
        "real_data = []\n",
        "\n",
        "number_words = 0\n",
        "store = 1\n",
        "for index in range(1,number,1):\n",
        "    # start of the for loopfor storing data\n",
        "    index = np.random.randint(len(X_test))\n",
        "    print(index)\n",
        "    if(index != 0):\n",
        "      count = len(X_test)\n",
        "    else:\n",
        "      count = count_\n",
        "    input_word = X_test[index]\n",
        "    index_number = index\n",
        "    predicted_word = hindi_output_word(net,input_word)\n",
        "\n",
        "    if Y_test[index][1:-1] == predicted_word and 1 == store:\n",
        "        real_data.append(\"Yes\")\n",
        "    else:\n",
        "        real_data.append(\"No\")        \n",
        "        \n",
        "    input_word_array.append(input_word)\n",
        "    count = number_words\n",
        "    correct_word.append(Y_test[index][1:-1])\n",
        "    count+=1\n",
        "    predicted_word_array.append(predicted_word)\n",
        "\n",
        "grid = {'Input Word': input_word_array, 'Correct Output' : correct_word, 'Output Word' : predicted_word_array, \"Exact Match\" : real_data}\n",
        "type(grid)\n",
        "# data into data frame\n",
        "pd.DataFrame(grid)\n",
        "\n",
        "df=pd.DataFrame(grid)\n",
        " \n",
        "df.to_csv('/content/drive/MyDrive/Assignment_3/vanilla_predictions.csv', index=False,header=True)\n",
        "\n",
        "pd.DataFrame(grid)"
      ],
      "metadata": {
        "id": "WvKEQ-U-Jxhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i5i856SNwXZ",
        "outputId": "8da79777-2025-4149-8563-1ee0c2c847b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}